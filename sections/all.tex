\documentclass[../main.tex]{subfiles}
\begin{document}
\section{Abstract}

Modern distributed infrastructures rely on reproducibility and traceability to ensure reliable and predictable operation across large fleets of servers. This thesis addresses that challenge in the context of NixOS-based systems deployed across multiple data centres.

The project designs and implements a host overview system that establishes a reliable mapping between Nix store paths and the Git revisions from which they were built. This mapping enables two key capabilities:
(1) Cluster consistency analysis, allowing operators to verify whether nodes across a cluster are running system images produced from the same Git commit; and
(2) Reproducibility verification, by detecting when distinct Git commits generate identical Nix store paths, thereby confirming that source changes do not affect the resulting system image.

Embedding commit hashes directly into system builds is infeasible, as it introduces recursive dependencies and breaks Nix reproducibility guarantees. Instead, this work develops a distributed data-collection and aggregation service that captures runtime system metadata across hundreds of servers and correlates it with information from the build pipeline.

The resulting system provides high-confidence provenance tracking for NixOS deployments and offers a foundation for improved observability, auditing, and debugging of large-scale Nix-based infrastructures.

\newpage

\section{Introduction}
\subsection{Context, Gap, Innovation and Evaluation}

DBC Digital operates a fleet of NixOS-based servers whose system configurations are specified, built, and deployed from a central Git repository. In such an environment, reproducibility and traceability are essential. Operators must be able to determine exactly which system image is running on each server and verify that these images originate from the same Git revision across nodes. For this reason,  accurate provenance linking Nix store paths to the Git revisions from which they were built is a prerequisite for reliable auditing and deployment validation.

Currently, this provenance is reconstructed through a fragmented process. The CI system generates a CSV file that maps built Nix store paths to Git commits. To gather runtime activation data, the CI server then uses a custom PHP-based tool that connects to each machine over SSH and retrieves logs produced by the activation script.


While this approach works in day-to-day operations, it has several limitations. It requires broad SSH access across the infrastructure, relies on CSV as a primary data store instead of a database, which limits querying and historical capabilities, in addition to being difficult to extend or integrate with other systems. As DBC Digital's needs evolve, these issues hinder scalability, security, and operability.

To address this gap, this project designs and implements a unified, secure host overview system called \textit{hostmap} that maintains a mapping between Nix store paths and the Git revisions that produced them. The system stores runtime activation data from across the infrastructure in a central service, and correlates them with mapping data received from the CI server. 

This enables two key capabilities: verifying cluster consistency by comparing the effective commit behind each node, and validating reproducibility by detecting cases where different commits produce identical store paths.

The proposed system is evaluated by comparing its reliability, security, and usability with the existing SSH-based setup. A successful solution will match the current tooling's functionality while significantly enhancing security, data quality, and long-term operability.

\subsection{Contributions}

\begin{itemize}
     \item A secure host overview service, called hostmap
     \item A scraper for the hosts' Nix activations
     \item A back end integrating with CI that saves commit to store-path mappings to the database
     \item A frontend for auditing current and historical system paths to git commit mappings
\end{itemize}


\newpage
\section{Problem Description and System Requirements}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.00\textwidth]{images/buildtime.drawio.pdf}
    \caption{Creating mappings between commit hashes and nix store paths}
    \label{fig:diagram-buildtime}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.00\textwidth]{images/runtime.drawio.pdf}
    \caption{Scraping activation data over SSH}
    \label{fig:diagram-runtime}
\end{figure}

Diagram~\ref{fig:diagram-runtime} shows that the PHP tool DBC Digital requires SSH access to all servers. This is a security concern. If the CI server is compromised, an attacker could gain privileged access to every host via the CI server, effectively bypassing firewall protections.

The CI Server performs SSH scraping, but it prioritizes building first and then scraping, which can result in delays of up to an hour in reflecting the latest activations in the CSV file.

The delays can be up to an hour. This is problematic for the infrastructure team at DBC. Since they use this tool to ensure that the hosts in a cluster are running the same version, these delays make version monitoring difficult. 

Creating mappings between commit hash and store path is out of scope for the project. My system will receive a mapping and be responsible for linking it with the correct host and activation data. DBC Digital has decided that its CI Server will create the mapping, since it is already building the system image for other purposes.

\subsection{Why do we need mappings}
Diagram~\ref{fig:diagram-buildtime} illustrates how the mappings are created. One might assume that a naiver, and more straightforward way to link a Nix store path to a Git commit would be to embed the commit hash directly into the Nix derivation that builds the system. However, this approach is infeasible due to the recursive dependency it introduces. Embedding the commit hash would require knowledge of the commit at build time, which in turn depends on the build process itself. This circular dependency breaks Nix's reproducibility guarantees and complicates the build process.

In addition to this recursive issue, we would also not be able to verify that a host has not changed between two commits if we embed the commit hash directly into the derivation, which is a key requirement for DBC Digital.

\subsection{System Requirements}

The system requirements are divided into functional and non-functional requirements.
\subsubsection{Functional Requirements}
\begin{itemize}
	\item The system must ingest activation events from all hosts
	\item The system must store mappings between store paths and Git revisions
	\item The system must allow end users to view the current revision per host
	\item The system must present hosts grouped by a custom key in the UI
\end{itemize}
\subsubsection{Non-Functional Requirements}
\begin{itemize}
	\item Security, the system must avoid the need for privileged access (as used in the current SSH-based solution) and must authenticate endpoint access where appropriate
	\item Reliability, must survive network interruptions
	\item Extensibility, new metadata fields can be added later
	\item Performance, must work well when deployed to 200 servers, each activating at most once per day on average.
	\item Maintainability, simple deployment
\end{itemize}


\subsection{Nix Package Manager and NixOS}
NixOS is a Linux distribution built with the Nix package manager, which enables declarative and reproducible systems. In Nix, the inputs required to build a package, e.g., source files, compilers, environment variables, libraries, and more, are specified in a \lstinline{.nix} file, and when using flakes (a newer workflow for nix), their exact versions are pinned in a \lstinline{flake.lock} file.

To understand Nix we can compare it to how pure function in functional programming works. A pure function has no hidden state and , it will, for the same input, return the same output.

Drawing inspiration from this, nix does the same for software builds. In your Nix expression, you list all of your required inputs, and for the same build inputs the build artifact will, regardless of where, how, or when it is built, always be the same.

The build inputs all appear in the final store path of the artifact under \lstinline{/nix/store/<unique-hash>-<name>}, and the unique hash is calculated from all the build inputs. If you change something small, such as an environment variable, the inputs will be altered and thus the hash of the build artifact will be changed. This ensures reproducible and deterministic builds. If the store path is identical, the contents are identical as well. 

To ensure that the inputs themselves(which are also Nix expressions) have not been altered, Nix downloads all of the sources and evaluates the nix expressions. If the output hash does not match the expected hash in the input, something unexpected has happened. The input might have been corrupted, or tampered with, and the build fails. 

\begin{figure}[h]
\begin{lstlisting}
flask==1.0.0
numpy==1.0.4
pillow==1.2.0
requests==1.3.1
cryptography==2.0.0
\end{lstlisting}
\caption{An example \texttt{requirements.txt} file that looks reproducible, but is not.}
\label{lst:reqfile}
\end{figure}


Contrast this with the approach of traditional package managers. There are many and they all share the same problem, e.g. npm, pip, Maven, etc. With pip one uses a requirements.txt file. Fig. \ref{lst:reqfile} looks reproducible at a first glance, since the exact versions are specified. There are however several problems. Two examples are: we do not know which version of python we need and libraries such as cryptography and requests depends on native system libraries, in this case OpenSSL. The version of OpenSSL is not specified.

These two problems may lead to the classic "it works on my machine" problem, and subtle bugs, that are hard to diagnose.

By treating builds like pure functions and embedding all inputs directly into the Nix store paths, Nix enables strong reproducibility guarantees. These ideas form one of the foundations for the system developed in this project, which relies on the deterministic nature of Nix builds to support reliable provenance tracking and auditing.

\subsection{Embedding git commit into Nix expression}

% explain why we can't do that here??? Or in 

\newpage
\section{Methodology}
% the methodology i have internalized
% should only describe iterative process, not what I did
\subsection{Gathering Requirements}

The project began with a phase where I gathered requirements. The first steps was looking at the current solution and understanding its limitations and how it worked.

In collaboration with the infrastructure team at DBC digital in particular Adam and Sarah, I worked out the requirements for the project to be considered successful. Among these criterias were eliminating the SSH access from the CI server to the server fleet, reducing delays in UI updates, and enabling some extensibility through the use of arbitrary metadata.

In addition, the scope of the project was limited to not include any changes to the CI server on my part. DBC digital ensured that any changes to that part of the system will be done by them. In more generic terms this can be described as my system is responsible for receiving the mappings between git commit hash and a store path but not for creating them.

This was great because this enabled a very clean and minimal interface between what I was responsible for and what they were responsible for. 


\subsection{Testing and Validation}
Testing is necessary to be able to prove that a system lives up to the requirements. A mix of testing methods were agreed upon, were one of the criterias is that it should at the very least mimic all of the functionality of the old system.


\subsection{Documentation and Traceability}

\newpage
\section{Process}
% how the methodology relates to the actual process, what worked?

\subsection{Developmental Method}
I Suggested they agree

We agreed to use an iterative process where one of the key goals was getting a MVP(minimal viable product) up and running as soon as possible, so that DBC Digital can set up their CI servers to send the mapping data to the backend. 

To make the process more iterative, and arrive at MVP earlier, we decided to put up an nginx proxy up to statically serve the activation log file that the old host overview system used, and use that when working on the first prototype of hostmap.

In addition to this, we decided that I will meet up at their offices two times a week, to make the development process more efficient.

----- 

hint at the solution in architectural and system design
\newpage
\section{Solution}
\subsection{Architectural and System Design}

When selecting the architecture I opted for a top down model where I first consider the overall system and then when decided on a good architecture I selected technologies. 

There were two important architectural decisions regarding what model to use for moving data to backend. 

When moving data from the server fleet to the backend we selected a pull based approach. This enables us to have a very basic HTTP server serving log file as the only thing the servers need to do. If we were to choose push-based approach the logic reciting on the server fleet would have to be more advanced. For example the servers in the fleet would individually need to be able to detect when a request has potentially silently failed and have a retry logic to send the data again. 

Moving to a pool-based approach means that only the backend will need to have this logic instead of hundreds of servers.

In addition to this having a pool based approach lets the backend decide on the load. If the approach is push based, potentially hundreds of servers could at once and data to the backend, overwhelming it. 

Thirdly we avoid the need for authentication. Since the back end fetches the data it can't just fetch from the service it want to fetch from, and the data it vectors is not sensitive so there is no need for authentication. With a pull based approach we would need authentication so that malicious actors cannot send junk data, and we avoid the need for handling and rotating API keys for hundreds of servers. 

When moving data from the CI server to the backend we selected a push-based approach. Several of the problems mentioned above goes away. There is only one CI server so handling of API keys is not a problem. The back and won't be overwhelmed by a single CI server. 

We also gain one thing. Since the ca server notes exactly when it has built the store path from the git commit, it can send the data immediately to them back end meaning that data will be fresher.

\newpage
\section{Evaluation}

\subsection{Evaluation Strategy}
The project was tested using a combination of local testning, integration testing against test servers for activation log fetching, testing correctness of matching mappings to fetched logs and end-user tests.

\subsection{Testing and Validation}
To make sure that the system fulfills the requirements a series of tests were conducted, at different levels for different parts of the system. 

\subsubsection{Unit Tests}
Unit tests were written for selected parts of the core of the system. Most of the time writing unit tests went into making sure the authentication of API keys was correctly implemented. These tests included handling malformed headers in authentication request, incorrect prefixes, and unexpected unexpected whitespace. 

While writing these tests, a vulnerabilty was discovered. The API key comparisons were open to side-channel attacks, since it used string equality comparison instead of constant-time equality comparison. Although this was not discovered through a test failing, the process of writting the tests made the issue apparent. The comparison was changed to use constant-time comparison which improved security in the final product, and aligned with one of the non-functional requirements. 

Some non-critical components were also tested, and here I wrote the tests before writing the implementation of the method. This was done for the method that shortens the system paths into easier to read text by removing some of the hash in the middle.

I also wrote unit tests for conversion between errors to ensure that the correct HTTP status code is returned upon error.

These methods all have in common that they are pure, and that is why the were selected for unit testing. Due to time constraints, and that the conversions DTO (Data Transfer Object) and model and back were trivial, I elected not to write unit tests for this mapping.


\subsubsection{Local Integration Tests}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.00\textwidth]{images/succesful-integration-test.png}
    \caption{Matching host's current store path (yellow), with the correct git commit and branch(blue)}
    \label{fig:match-result}
\end{figure}


The most complex logic in the back end is matching the each host with its latest activation with the correct Git commit (revision) and branch, (see Appendix~\ref{appendix:matching-sql}). It must select the most recent activation for each host, join it with all commit mappings for the activation's store path. Among these mappings, the query selects the row with the earliest \lstinline{linked_at} timestamp, but prioritizing rows with a branch called 'master' over mapping rows.

Since this logic involves ordering rows, multiple times on multiple arguments, and have a non-trivial \lstinline{JOIN}, and since unit tests generally are never enough to test SQL queries, correctness needs to be validated through integration tests. 

The integration test was created by writing a carefully constructed dummy data seeding script. I designed the seed data manually, and thought through what the expected behaviour should be and then explictly marked the correct and wrong commit hashes of the mapping. For the entry with hostname \textit{test-1} the correct hash, that should display, was marked \textit{3CORRECT}, where as the other two, a later master branch commit, and an earlier commit from a branch not called master, were called '00latest' and '555WRONG'. This made it immediately clear when testing, wheter the system had mapped correctly.

After seeding the database, the back end part of the system was started and then I could use the UI to confirm that the expected commits appeared in the view, as shown in Figure~\ref{fig:match-result}.

\subsubsection{CI Mapping Simulation with Curl}

\subsubsection{Security Considerations}

\subsubsection{Data Freshness Improvements}

\subsection{Performance Considerations SKIP??}

\subsection{User Evaluation}

\newpage

\section{Discussion}
\begin{figure}[h]
\centering
\begin{lstlisting}
// before - vulnerable to timing differences
client_api_key == server_api_key;

// after - constant-time comparison
bool::from(client_api_key.as_bytes().ct_eq(server_api_key.as_bytes()))
\end{lstlisting}
\caption{Replacing the string comparison with a constant-time comparison to avoid timing-based side channels.}
\label{fig:constant-time-comparison}
\end{figure}
While writing unit tests for the API-key validation, I realised that the comparison logic was vulnerable to side-channel attacks, as shown in Figure~\ref{fig:constant-time-comparison}. This issue was not detected by a test failure, instead the process of writing the unit tests made the potential side-channel risk apparent. This lead me to replacing string comparison with a constant-time comparison function to avoid timing attacks, thus aligning both with non-functional requirements and coding best practices.

\appendix
\section{SQL Query for Host with Latest Activation to Commit Matching}
\label{appendix:matching-sql}
\begin{figure}
\begin{lstlisting}[language=SQL]
WITH latest AS (
  SELECT
  DISTINCT ON (ac.hostname)
    ac.activation_id, ac.activated_at,
    ac.username, ac.store_path,
    ac.activation_type, ac.hostname
  FROM activation ac
  ORDER BY ac.hostname, ac.activated_at DESC
)
SELECT DISTINCT ON(l.hostname) 
  l.activation_id, l.activated_at,
  l.username, l.store_path,
  l.activation_type, l.hostname,
  ngl.commit_hash, ngl.branch
  FROM latest l
  LEFT JOIN nix_git_link ngl ON ngl.store_path = l.store_path
  ORDER BY l.hostname, ngl.branch = 'master' desc, ngl.linked_at asc
;
\end{lstlisting}
\caption{SQL query used by the back end to match host with latest activation with Git commit mappings.}
\end{figure}
\end{document}

