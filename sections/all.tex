\documentclass[../main.tex]{subfiles}
\begin{document}
\section{Abstract}

Modern distributed infrastructures rely on reproducibility and traceability to ensure reliable and predictable operation across large fleets of servers. This thesis addresses that challenge in the context of NixOS-based systems deployed across multiple data centres.

The project designs and implements a host overview system that establishes a reliable mapping between Nix store paths and the Git revisions from which they were built. This mapping enables two key capabilities:
(1) Cluster consistency analysis, allowing operators to verify whether nodes across a cluster are running system images produced from the same Git commit; and
(2) Reproducibility verification, by detecting when distinct Git commits generate identical Nix store paths, thereby confirming that source changes do not affect the resulting system image.

Embedding commit hashes directly into system builds is infeasible, as it introduces recursive dependencies and breaks Nix reproducibility guarantees. Instead, this work develops a distributed data-collection and aggregation service that captures runtime system metadata across hundreds of servers and correlates it with information from the build pipeline.

The resulting system provides high-confidence provenance tracking for NixOS deployments and offers a foundation for improved observability, auditing, and debugging of large-scale Nix-based infrastructures.

\newpage

\section{Introduction}
\subsection{Context, Gap, Innovation and Evaluation}

DBC Digital operates a fleet of NixOS-based servers whose system configurations are specified, built, and deployed from a central Git repository. In such an environment, reproducibility and traceability are essential. Operators must be able to determine exactly which system image is running on each server and verify that these images originate from the same Git revision across nodes. For this reason,  accurate provenance linking Nix store paths to the Git revisions from which they were built is a prerequisite for reliable auditing and deployment validation.

Currently, this provenance is reconstructed through a fragmented process. The CI system generates a CSV file that maps built Nix store paths to Git commits. To gather runtime activation data, the CI server then uses a custom PHP-based tool that connects to each machine over SSH and retrieves logs produced by the activation script.


While this approach works in day-to-day operations, it has several limitations. It requires broad SSH access across the infrastructure, relies on CSV as a primary data store instead of a database, which limits querying and historical capabilities, in addition to being difficult to extend or integrate with other systems. As DBC Digital's needs evolve, these issues hinder scalability, security, and operability.

To address this gap, this project designs and implements a unified, secure host overview system called \textit{hostmap} that maintains a mapping between Nix store paths and the Git revisions that produced them. The system stores runtime activation data from across the infrastructure in a central service, and correlates them with mapping data received from the CI server. 

This enables two key capabilities: verifying cluster consistency by comparing the effective commit behind each node, and validating reproducibility by detecting cases where different commits produce identical store paths.

The proposed system is evaluated by comparing its reliability, security, and usability with the existing SSH-based setup. A successful solution will match the current tooling's functionality while significantly enhancing security, data quality, and long-term operability.

\subsection{Contributions}

\begin{itemize}
     \item A secure host overview service, called hostmap
     \item A scraper for the hosts' Nix activations
     \item A back end integrating with CI that saves commit to store-path mappings to the database
     \item A frontend for auditing current and historical system paths to git commit mappings
\end{itemize}


\newpage
\section{Problem Description and System Requirements}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.00\textwidth]{images/buildtime.drawio.pdf}
    \caption{Creating mappings between commit hashes and nix store paths}
    \label{fig:diagram-buildtime}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.00\textwidth]{images/runtime.drawio.pdf}
    \caption{Scraping activation data over SSH}
    \label{fig:diagram-runtime}
\end{figure}

Diagram~\ref{fig:diagram-runtime} shows that the PHP tool DBC Digital requires SSH access to all servers. This is a security concern. If the CI server is compromised, an attacker could gain privileged access to every host via the CI server, effectively bypassing firewall protections.

The CI Server performs SSH scraping, but it prioritizes building first and then scraping, which can result in delays of up to an hour in reflecting the latest activations in the CSV file.

The delays can be up to an hour. This is problematic for the infrastructure team at DBC. Since they use this tool to ensure that the hosts in a cluster are running the same version, these delays make version monitoring difficult. 

Creating mappings between commit hash and store path is out of scope for the project. My system will receive a mapping and be responsible for linking it with the correct host and activation data. DBC Digital has decided that its CI Server will create the mapping, since it is already building the system image for other purposes.

\subsection{Why do we need mappings}
Diagram~\ref{fig:diagram-buildtime} illustrates how the mappings are created. One might assume that a naiver, and more straightforward way to link a Nix store path to a Git commit would be to embed the commit hash directly into the Nix derivation that builds the system. However, this approach is infeasible due to the recursive dependency it introduces. Embedding the commit hash would require knowledge of the commit at build time, which in turn depends on the build process itself. This circular dependency breaks Nix's reproducibility guarantees and complicates the build process.

In addition to this recursive issue, we would also not be able to verify that a host has not changed between two commits if we embed the commit hash directly into the derivation, which is a key requirement for DBC Digital.

\subsection{System Requirements}

The system requirements are divided into functional and non-functional requirements.
\subsubsection{Functional Requirements:}
\begin{itemize}
	\item The system must ingest activation events from all hosts
	\item The system must store mappings between store paths and Git revisions
	\item The system must allow operators to query the current revision per host
	\item The system must present hosts grouped into clusters in the UI
\end{itemize}
\subsubsection{Non-Functional Requirements:}
\begin{itemize}
	\item Security, no SSH needed from CI to servers
	\item Reliability, must survive network interruptions
	\item Extensibility, new metadata fields can be added later
	\item Performance, must work well when deployed to 200 servers, each activating at most once per day on average.
	\item Maintainability, simple deployment
\end{itemize}

\newpage
\section{Theoretical Background (Optional)}
\newpage
\section{Project Plan}
\subsection{Methodology}
\newpage
\section{Solution}
\newpage
\section{Implementation}
\newpage
\section{Evaluation}
\newpage
\section{Discussion}
\end{document}

% # ⭐ **1. Abstract**
% 
% **Length:** ~½ page maximum
% **Purpose:** Give the reader the *entire story* in compressed form.
% 
% Include **four things** (CGIE model):
% 
% 1. **Context:**
% 
%    * DBC Digital uses NixOS; provenance matters for reproducibility and auditing.
% 
% 2. **Gap:**
% 
%    * Current provenance tracking is fragmented (CSV + SSH + PHP), insecure, and not scalable.
% 
% 3. **Innovation:**
% 
%    * You built *hostmap*: a secure, unified system linking Nix store paths to Git commits.
% 
% 4. **Evaluation:**
% 
%    * Summarize results: improved reliability, security, data quality; matches or exceeds old system.
% 
% **Do *not* include:** code, citations, detailed architecture.
% 
% ---
% 
% # ⭐ **2. Introduction**
% 
% This section *motivates* the project and gives the reader an overview.
% 
% ### 2.1 **Context, Gap, Innovation and Evaluation (CGIE)**
% 
% * Introduce the infrastructure and why provenance matters.
% * Explain shortcomings of the existing system.
% * Present your high-level contribution (hostmap).
% * Describe how the solution is evaluated (reliability + security + usability).
% 
% This subsection should read like a **story**, not like a requirements list.
% 
% ### 2.2 **Contributions**
% 
% Make a bullet list of the *new things you built*. Examples:
% 
% * A secure host overview service (hostmap)
% * An activation scraper and ingestion pipeline
% * A CI integration producing commit→store-path mappings
% * A UI or API for cluster consistency checks
% * Documentation suitable for internal or open-source use
% * A reproducibility verification mechanism
% 
% ***Important:*** Contributions should be *sharp, measurable, technical*.
% 
% ---
% 
% # ⭐ **3. Problem Description and System Requirements**
% 
% This is where you formally state the *problem you are solving* and what a solution must do.
% 
% ### **Problem Description**
% 
% * Explain the exact challenge: provenance reconstruction is fragmented, insecure, and hard to query.
% * Clarify what operators *cannot* do today (e.g., cluster consistency verification).
% * Define the scope: you are not redesigning the entire CI system; only provenance tracking.
% 
% ### **System Requirements**
% 
% Use two parts:
% 
% #### **Functional Requirements (FR):**
% 
% Examples:
% 
% * The system must ingest activation events from all hosts.
% * The system must store mappings between store paths and Git revisions.
% * The system must allow operators to query the current revision per host.
% * The system must detect when nodes in a cluster run different revisions.
% 
% #### **Non-Functional Requirements (NFR):**
% 
% Examples:
% 
% * Security (no SSH needed from CI → servers)
% * Reliability (must survive network interruptions)
% * Extensibility (new data fields can be added later)
% * Performance (support X servers, Y activations/day)
% * Maintainability (simple deployment, readable code)
% 
% This section sets **success criteria** for your solution.
% 
% ---
% 
% # ⭐ **4. Theoretical Background (Optional)**
% 
% Include **only if needed**.
% This section explains concepts the reader must understand *before* reading the solution.
% 
% Examples relevant to your thesis:
% 
% * How NixOS builds systems (system closure, store paths, derivations)
% * Why Git commit metadata cannot be embedded in Nix derivations (infinite recursion)
% * Event sourcing / logging theory (if relevant)
% * Software provenance concepts
% * Basics of distributed systems observability
% 
% If the reader needs these to understand later sections → include them.
% If not → keep this section short or omit it entirely.
% 
% ---
% 
% # ⭐ **5. Project Plan**
% 
% Describe how you structured and executed the project.
% 
% ### Include:
% 
% * Timeline (e.g., Gantt chart or milestones)
% * Major phases (research, design, prototype, implementation, evaluation)
% * Tools used for planning (Git, Notion, Jira, etc.)
% * Any collaboration or supervision constraints
% 
% **This section is not technical**—it describes *how* you organized the work.
% 
% ### 5.1 **Methodology**
% 
% Explain your scientific and engineering approach.
% 
% Examples:
% 
% * CGIE as a guiding framework
% * Iterative development (e.g., Agile-like workflow)
% * How design decisions were evaluated
% * Data collection methods for evaluation
% * Why you chose Rust, Axum, SQL, Nix, etc.
% 
% ---
% 
% # ⭐ **6. Solution**
% 
% This is the **high-level design / architecture**, NOT the implementation.
% 
% ### Include:
% 
% * System overview diagram
% * Architecture components:
% 
%   * Activation logger
%   * API / scraper
%   * Database
%   * CI-mapping ingestion
%   * Web UI / CLI
% * How data flows through the system
% * Security design decisions
% * Why this architecture satisfies the requirements
% * Alternatives considered (and why rejected)
% 
% Think of this section as your **design document**.
% 
% ---
% 
% # ⭐ **7. Implementation**
% 
% This is where you go *into the details*.
% 
% ### Include:
% 
% * Technologies used (Rust, Axum, SQLx, Tera, etc.)
% * Database schema + migrations
% * API endpoints
% * Data structures
% * CI integration mechanism
% * How activation data is parsed and validated
% * Error handling
% * Deployment considerations
% * Code snippets (when they clarify something)
% 
% Avoid huge code dumps—summarize with diagrams and short snippets instead.
% 
% ---
% 
% # ⭐ **8. Evaluation**
% 
% This shows whether your system works, based on the requirements from Section 3.
% 
% ### Include measurable results:
% 
% * Correctness of store-path → commit mappings
% * How well the system reconstructs activation history
% * Performance tests (ingestion speed, query speed)
% * Security improvements (no SSH from CI → servers)
% * Comparison table: *old vs new system*
% 
% ### Also include qualitative evaluation:
% 
% * Operator usability
% * Maintainability
% * Extensibility
% * Deployment simplicity
% 
% Tie all results back to the requirements you defined earlier.
% 
% ---
% 
% # **9. Discussion**
% 
% Here you reflect critically.
% 
% ### Include:
% 
% * Limitations of your system
% * Unexpected design challenges
% * What you would do differently with more time
% * How the system could evolve (future work)
% * Risks and assumptions
% * Generalizability — can this be used outside DBC Digital?
% 
% This section shows **maturity and academic reflection**.
% 
% 
